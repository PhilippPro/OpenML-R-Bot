

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm,amsmath,amssymb,amstext} 
\usepackage{booktabs}
\usepackage{array}
 
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\usepackage{hyperref}
\hypersetup{%
  colorlinks=true, 
  breaklinks=true, 
  urlcolor=blue, 
  linkcolor=blue, 
  citecolor=blue, 
}

\usepackage{etoolbox}
\appto\UrlBreaks{\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j
\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w
\do\x\do\y\do\z}


\usepackage{nicefrac}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{setspace}


\begin{document}


\title{Automatic Exploration of Machine Learning Experiments on OpenML}
\author{by Daniel Kühn*, Philipp Probst*, Janek Thomas and Bernd Bischl}

\maketitle

\section*{Abstract}
Understanding the influence of hyperparameters on the performance of a machine learning algorithm is an important part of finding a well performing and adequately tuned algorithm for a given dataset. As to date no dataset exists to support this required understanding for state of the art algorithms like gradient boosting or random forest, this paper presents a large and open dataset for this problem (see \citep{Probst2018} to access the dataset). The dataset contains the performance results  (AUC, accuracy and Brier score) of six different machine learning algorithms, the used hyperparameters that were set by a random search, runtimes and meta-data for each dataset. It can be used for meta-learning, finding good defaults, measuring the tunability of algorithms and hyperparameters, benchmarking of algorithms and other tasks mainly related to hyperparameters. %The contained algorithms are elastic net (glmnet R-package), decision tree (rpart), k-nearest neighbors (kknn), support vector machine (e1071), random forest (ranger) and extreme gradient boosting (xgboost).


\section{Introduction}

When applying machine learning on real world datasets, users have to choose from a large selection of different machine learning algorithms with many of these algorithms offering a set of hyperparameters, which can be specified by the user and can have a significant influence on the performance of the algorithm. Since there is no free lunch in algorithm selection and one can not expect one algorithm to outperform all the others \citep{Wolpert2001}, a crucial question practitioners have to face on a daily basis therefore is the selection of the "right" algorithm with the "right" hyperparameters for a given dataset. This problem is not easy to solve, because choices are many, the evaluation of a single machine learning run usally is computationally expensive and the hyperparameter-space is complex \citep{Claesen2015}. A usual approach is to run a tuning algorithm like bayesian optimization to find the best hyperparameter setting. While showing promising results, bayesian optimization and other hyperparameter optimization techniques require a large overhead. 

Meta-learning tries to decrease this overhead \citep{Feurer2015}, by using information of previous algorithm runs on other datasets. A requirement for this, is 
to have a meta-learning dataset, that contains the information of previous runs. With this paper we provide a open accesible dataset that contains information of 
previous runs of six different machine learning algorithms on 38 classification datasets. As large datasets like Imagenet \citep{ImageNet2009} have shown to improve the progress of machine learning, we hope to support the development of new meta-learning algorithms with this dataset. 

We first describe how we created such a dataset by executing random machine learning experiments and storing the results on OpenML \citep{OpenML2013}, an open source database for machine learning problems. Then the possibilities of accessing this dataset are shortly presented. Finally we briefly discuss potential usage of the dataset. 

\section{Related literature}

There is quite a few literature that use meta-learning approaches. 
A successful example is auto-sklearn \citep{Feurer20152} which is a automatic machine learning algorithm that uses information of previous experiments on other datasets. They use a meta-learning approach for warmstarting their tuning algorithm. 

On the other hand, to the best of our knowledge there are currently no papers and projects that provide data for meta-learning approaches, like we do in this paper. The only relevant paper we could found is the paper by \citet{Reif2012ACD} which presents a dataset for meta-learning based on 83 datasets, six classification algorithms,  and 49 meta-features. The dataset is not anymore available on the university wegpage as the researcher does not work there anymore. We tried to circumvent this potential problem by uploading our results on figshare which is meant to be a durable and open platform for storing and sharing data. 

%Other related projects and papers (e.g. AutoWeka) -> Gibt es Datensatz dazu?
%Dataset repositories: 
%- \citep{Dua2017} -> Ist nicht passend zu unserem Projekt sondern höchstens Alternative/Vorgänger von OpenML?

%Similar datasets:
%\citep{Reif2012ACD} describe a creation of a similar dataset in their paper. -> Paper ist doof, da Link nicht mehr funktioniert und nicht mal die Anzahl an Beobachtungen gegeben ist...

\section{Creating the dataset}

\subsection{Algorithms}

To create the dataset six supervised machine learning algorithms were used in R with predefined ranges for their relevant hyperparameters and they were run on 38 classification tasks. 
The following frequently used algorithms were chosen: elastic net (\texttt{glmnet}), decision tree (\texttt{rpart}), k-nearest neighbors (\texttt{kknn}), support vector machines (\texttt{svm}), random forest (\texttt{ranger}) and gradient boosting (\texttt{xgboost}). These algorithms cover a broad range of approaches to machine learning. For each algorithm the available hyperparameters were explored in a predefined range (see table~\ref{tab:parameter}). Values of some of these ranges were transformed by the function found in column \textit{trafo} to explore the range in a non-uniform manner. This is useful, if, for example, minor changes in a hyperparameter are not expected to have a significant impact on the performance of an algorithm.

<<parameter, echo = FALSE, results="asis",  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
library(xtable)
suppressWarnings(library(mlr))
load("./data/parameter_settings.RData")
df_total = data.frame()
index = numeric()
for (i in 1:6) {
  df = do.call(rbind, lrn.par.set[[i]]$param.set$pars)[,c("id", "type", "lower", "upper", "trafo")]
  if (is.null(nrow(df))) {
    df$trafo = NA 
    df = data.frame(df)
    colnames(df)[1] = "hyperparameter"
  } else {
    if (!all(data.frame(df)$trafo == "NULL")) {
      df = data.frame(df)
      df$trafo = lapply(df$trafo, function(x) if (!is.null(x)) paste0("$", deparse(x)[2], "$"))
      df$type[df$type == "numericvector"] = "numeric"
    } else { 
      df = data.frame(df)
      #df = subset(df, select = -trafo) 
    }
    colnames(df)[1] = "hyperparameter"
    df[df=="NULL"] = NA
  }
  #print(df)
  algorithm = c(getLearnerShortName(lrn.par.set[[i]]$learner), rep("", nrow(df)-1))
  df = cbind(algorithm, df)
  index[i] = nrow(df)
  df_total = rbind(df_total, df)
}
df_total$trafo$mtry = c("$x \\cdot p$")
df_total$trafo$min.node.size = c("$n^x$")
df_total$hyperparameter = gsub("_", "\\_", df_total$hyperparameter, fixed = TRUE)

#df_total
  print(xtable(df_total, caption = "Hyperparameters of the algorithms. $p$ refers to the number of variables and $n$ to the 
    number of observations.", digits = 0, label = "tab:parameter"), 
    NA.string = "-",sanitize.text.function=function(x){gsub("_", "_",x, fixed = TRUE)}, 
    include.rownames=FALSE, hline.after = c(0, cumsum(index)))
@

\subsection{Datasets}

These algorithms are run on a subset of the OpenML100 Benchmark suite \citep{Bischl2017}, which consists of 100 classification datasets
carefully curated from the thousands of datasets available on OpenML. We only include datasets without 
missing data and with binary outcome resulting in 38 datasets.
The datasets with their specific characteristics can be found in table~\ref{tab:datasets}.  

<<datasets, echo = FALSE, results="asis",  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
  library(OpenML)
  tasks = listOMLTasks(number.of.classes = 2L, number.of.missing.values = 0, 
    tag = "study_14", estimation.procedure = "10-fold Crossvalidation", status = "active")
  tasks2 = listOMLTasks(number.of.classes = 2L, number.of.missing.values = 0, 
    tag = "study_14", estimation.procedure = "10-fold Crossvalidation", status = "deactivated")
  tasks = rbind(tasks, tasks2)
  tasks = tasks[, c("data.id", "name", "number.of.instances", "number.of.features", "majority.class.size", 
    "number.of.numeric.features", "number.of.symbolic.features")]
  #tasks$majority.class.size = tasks$majority.class.size / tasks$number.of.instances
  colnames(tasks) = c("Data_id", "Name", "nObs", "nFeat", "majPerc", "numFeat", "catFeat")
  tasks$majPerc = tasks$majPerc/tasks$nObs
  
  load("./data/nr_results.RData")
  
  nr_results = nr_results[,which(colnames(nr_results) %in% sort(tasks$Data_id))]
  # delete these results from the database?
  # musk with easy perfect classification and two test datasets
  tasks = tasks[which(sort(tasks$Data_id) %in% colnames(nr_results)), ]
  # no results for some datasets
  tasks = tasks[order(tasks$Data_id),]
  #tasks$Data_id == colnames(nr_results)
  tasks = cbind(tasks, t(nr_results))
  colnames(tasks)[8:13] = c("glmnet", "rpart", "kknn", "svm", "ranger", "xgboost")
  tab = tasks[, c("Data_id", "Name", "nObs", "nFeat", "majPerc", "numFeat", "catFeat")]
  
  print(xtable(tab, caption = "Included datasets with meta-data. \\textit{nObs} are the number of observations, \\textit{nFeat} the number of Features, \\textit{majPerc} the percentage of observations with the most common class, \\textit{numFeat} the number of numeric features and \\textit{catFeat} the number of categorical features.", digits = 2, label = "tab:datasets"), digits = 0, include.rownames=FALSE, size = "scriptsize")
@

\subsection{Execution of the bot}

Following the search paradigm of random search the bot iteratively executes several steps:
\begin{enumerate}
\item Randomly draw one of the six algorithms
\item Randomly draw a hyperparameter setting of the chosen algorithm
\item Randomly draw one of the 38 binary classification benchmark datasets
\item Download the dataset from OpenML
\item Benchmark the specified algorithm on the specified dataset with 10-fold cross-validation (the standardized splits for the cross-validation are specified by OpenML)
\item Upload the benchmark results with hyperparameters, performance measures and time measurements to OpenML with the tag \texttt{mlrRandomBot} used for identification
\end{enumerate}


An advantage of using random search, instead of for example grid search is, that further experiments can be easily added to the existing ones. For example, one could easily increase the hyperparameter space for a specific algorithm or add a complete new algorithm by just adding new experiments to the existing ones. 

The code for the bot can be found on GitHub (\url{https://GitHub.com/ja-thomas/OMLbots}). Further algorithms can be easily added by adding an algorithm with specific hyperparameters in the \texttt{R/botSetLearnerParamPairs.R} file of the GitHub repository. The main function \texttt{runBot} executes the bot with specific settings (number of experiments, temporary file, sample configurations, etc.).

The R packages \texttt{mlr} \citep{Bischl2016} 
and \texttt{OpenML} \citep{Casalicchio2017} were used for the whole process. 


\subsection*{Extraction of results}

After having run more than 6 million benchmark experiments the results of the bot are downloaded from OpenML. 
Because of technical reasons on one dataset (data.id = 4135) all algorithms except of \texttt{rpart}
and \texttt{ranger} provide errors, so we exclude it and 38 datasets are left.

For each of the algorithms we only take 500000 experiments to obtain a dataset which is balanced regarding the experiment runs.  
They are chosen by the following procedure: for each algorithm, a threshold $B$ is set (see below) and, if the number of results for a dataset exceeds $B$,  we draw randomly $B$ of the results obtained for this algorithm and this dataset. For each algorithm, the threshold value $B$ is chosen for each algorithm separately to exactly obtain 500000 results for each algorithm. 

For \texttt{kknn} we only executed 30 experiments per dataset because this number of experiments is high enough
to cover the hyperparameter space (that only consists of the parameter $k$ for $k \in \{1,...,30\}$) appropriately, resulting in 1140 experiments.
In total this results in around 2.5 million experiments.

The distribution of the runs on the datasets and algorithms can be seen in table~\ref{tab:datasets2}.

<<tbl_results, echo = FALSE, results="asis",  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
tab = tasks[, c("Data_id", "glmnet", "rpart", "kknn", "svm", "ranger", "xgboost")]
  tab = cbind(tab, rowSums(tab[,2:7]))
  tab = rbind(tab, colSums(tab))
  tab[nrow(tab),1] = "Total"
  colnames(tab)[ncol(tab)] = "Total"

  print(xtable(tab, caption = "Number of results by dataset and algorithm", digits = 0, label = "tab:datasets2"), digits = 0, include.rownames=FALSE, size = "scriptsize", hline.after = c(-1, 0, nrow(tab)-1, nrow(tab)))
@

\section{Access to the benchmark results}

The results of the benchmarks can be accessed in different ways:

\begin{itemize}
\item The easiest way to access them is to go to the figshare repository \citep{Probst2018} and
download the \texttt{.csv} files or the \texttt{.RData} file. 
\item Alternatively the code for the extraction of the data from the nightly database snapshot of OpenML can 
be found here: \url{https://github.com/ja-thomas/OMLbots/blob/master/snapshot_database/database_extraction.R}
\end{itemize}

\section{Potential usage of the results and discussion}

The results can be used to discover effects of the hyperparameters on performances 
of the different algorithms on different datasets. 

This can be used to:
\begin{itemize}
\item Find good defaults for the algorithms that work well on many datasets
\item Measure differences between the algorithms
\item Optimize tuning algorithms:
\begin{itemize}
\item Measure the tunability of algorithms and find out which parameters should be tuned \citep[see][]{Probst20182}
\item Use the results to get priors for tuning algorithms - in which regions of the 
hyperparameter space should be searched with higher probability?
\end{itemize}
\item Meta-Learning: Train models that based on dataset characteristics and 
possibly time limitations propose hyperparameter settings that perform good 
on a specific dataset
\end{itemize}

A potential weakness of this dataset is that the dimension of the hyperparameters for, for example, \texttt{xgboost} is very high and the number of experiments is not high enough to explore the regions appropriately, especially regions where very high performance can be achieved. This could potentially be improved by using a tuning algorithm and by adding the results of the tuning steps to the existing database. 




\bibliography{bot}
 
\end{document}