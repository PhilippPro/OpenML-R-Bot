% COMMENT: The code that generates the file and tables is available at https://github.com/PhilippPro/OpenML-R-Bot/ in the file OpenML-R-Bot.rnw
%

\documentclass[authoryear]{elsarticle}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{}%
 \let\@evenfoot\@oddfoot}
\makeatother

% \definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
% \newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
% \newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
% \newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
% \newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
% \newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
% \newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
% \newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
% \newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
% \newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
% \let\hlipl\hlkwb

% \usepackage{framed}
% \makeatletter
% \newenvironment{kframe}{%
%  \def\at@end@of@kframe{}%
%  \ifinner\ifhmode%
%   \def\at@end@of@kframe{\end{minipage}}%
%   \begin{minipage}{\columnwidth}%
%  \fi\fi%
%  \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
%  \colorbox{shadecolor}{##1}\hskip-\fboxsep
%      % There is no \\@totalrightmargin, so:
%      \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
%  \MakeFramed {\advance\hsize-\width
%    \@totalleftmargin\z@ \linewidth\hsize
%    \@setminipage}}%
%  {\par\unskip\endMakeFramed%
%  \at@end@of@kframe}
% \makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
%\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper,%bindingoffset=0.2in,
            left=1.4in,right=1.4in,top=1.4in,bottom=1.4in%,footskip=.25in
            ]{geometry}

\usepackage{amsthm,amsmath,amssymb,amstext}
\usepackage{booktabs}
\usepackage{array}

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\usepackage{hyperref}
\hypersetup{%
  colorlinks=true,
  breaklinks=true,
  urlcolor=blue,
  linkcolor=blue,
  citecolor=blue,
}

\usepackage{etoolbox}
\appto\UrlBreaks{\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j
\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w
\do\x\do\y\do\z}


\usepackage{nicefrac}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{setspace}
%\usepackage{authblk}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{Automatic Exploration of Machine Learning Experiments on OpenML}
%\author{by Daniel Kühn*, Philipp Probst*, Janek Thomas and Bernd Bischl}


\author[1]{Daniel Kühn*}
\ead{daniel.kuehn.87@gmail.com}
\author[1]{Philipp Probst*}
\ead{philipp\_probst@gmx.de}
\author[1]{Janek Thomas}
\ead{janek.thomas@stat.uni-muenchen.de}
\author[1]{Bernd Bischl}
\ead{bernd\_bischl@gmx.net}

\address[1]{Ludwig-Maximilians-Universit\"at M\"unchen, Germany}



\begin{abstract}
Understanding the influence of hyperparameters on the performance of a machine learning algorithm is an important scientific topic in itself and can help to improve automatic hyperparameter tuning procedures.
Unfortunately, experimental meta data for this purpose is still rare.
This paper presents a large, free and open dataset addressing this problem, containing results
on 38 OpenML data sets, six different machine learning algorithms and many different hyperparameter configurations.
Results where generated by an automated random sampling strategy, termed the \textit{OpenML Random Bot}.
Each algorithm was cross-validated up to $20.000$ times per dataset with different hyperparameters settings, resulting in a meta dataset of around $2.5$ million experiments overall.

%Such data can be invaluable for meta-learning, benchmarking and other tasks related to hyperparameters like finding good defaults or measuring the tunability of algorithms and hyperparameters.

%The contained algorithms are elastic net (glmnet R-package), decision tree (rpart), k-nearest neighbors (kknn), support vector machine (e1071), random forest (ranger) and extreme gradient boosting (xgboost).
\end{abstract}

\maketitle

\section{Introduction}

When applying machine learning algorithms on real world datasets, users have to choose from a large selection of different algorithms with many of them offering a set of hyperparameters to control algorithmic performance.
Although sometimes default values exist, there is no agreed upon principle for their definition
(but see our recent work in in \citep{Probst2018} for a potential approach).
Automatic tuning of such parameters is a possible solution \citep{Claesen2015}, but comes with a considerable computational burden.

%This problem is very hard to solve, since many algorithms exist, the evaluation of a single machine learning run is often computationally expensive and the hyperparameter-space is complex \
%The usual approach is to run a hyperparameter tuning algorithm such as random search, grid search or Bayesian optimization \citep{snoek2012practical} to find the best hyperparameter setting.
%These methods have the drawback that a large number of runs might be necessary which can result in very high computational costs.

%they can be suboptimal and need to be specified by the user which often has large influence on the performance of the algorithm.
%The \textit{no free lunch theorem} \citep{Wolpert2001} states that in algorithm selection one algorithm can not consistently outperform all others algorithms for every dataset, so a crucial question practitioners have to face on a daily basis therefore is the selection of the best algorithm with optimal hyperparameters for a given dataset.

Meta-learning tries to decrease this cost \citep{Feurer2015}, by reusing information of previous runs of the algorithm on similar datasets, which obviously requires access to such prior empirical results.
With this paper we provide a freely accessible meta dataset that contains around $2.5$ million runs of six different machine learning algorithms on $38$ classification datasets.

Large, freely available datasets like Imagenet \citep{ImageNet2009} are important for the progress of machine learning, we hope to support developments in the area of meta-learning and benchmarking, meta-learning and hyperparameter tuning with our work here.

While similar meta-datasets have been created in the past, we were not able to access them by the links provided in their respective papers:
\citet{Smith2014} provides a repository with Weka-based machine learning experiments on 72 data sets, 9 machine learning algorithms, 10 hyperparameter settings for each algorithm, and several meta-features of each data set.
\citet{Reif2012ACD} created a meta-dataset based on machine learning experiments on 83 datasets, 6 classification algorithms, and 49 meta features.

In this paper, we describe our experimental setup, to specify how our meta-dataset is created by running random machine learning experiments through the OpenML platform \citep{OpenML2013} and how to access our results.

\section{Considered ML data sets, algorithms and hyperparameters}

To create the meta dataset, six supervised machine learning algorithms are run on 38 classification tasks.
%elastic net (\texttt{glmnet}), decision tree (\texttt{rpart}), k-nearest neighbors (\texttt{kknn}), support vector machines (\texttt{svm}), random forest (\texttt{ranger}) and gradient boosting (\texttt{xgboost}).
For each algorithm the available hyperparameters are explored in a predefined range (see Table~\ref{tab:parameter}).
Some of these hyperparameters are transformed by the function found in column \textit{trafo} of Table~\ref{tab:parameter}
to allow non-uniform sampling, a usual procedure in tuning.

%This is an often performed procedure, if e.g. minor changes for bigger values of a hyperparameter are not expected to have a significant impact on the performance of an algorithm.

<<parameter, echo = FALSE, results='asis',  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
library(xtable)
suppressWarnings(library(mlr))
load("./data/parameter_settings.RData")
df_total = data.frame()
index = numeric()
for (i in 1:6) {
  df = do.call(rbind, lrn.par.set[[i]]$param.set$pars)[,c("id", "type", "lower", "upper", "trafo")]
  if (is.null(nrow(df))) {
    df$trafo = NA
    df = data.frame(df)
    colnames(df)[1] = "hyperparameter"
  } else {
    if (!all(data.frame(df)$trafo == "NULL")) {
      df = data.frame(df)
      df$trafo = lapply(df$trafo, function(x) if (!is.null(x)) paste0("$", deparse(x)[2], "$"))
      df$type[df$type == "numericvector"] = "numeric"
    } else {
      df = data.frame(df)
      #df = subset(df, select = -trafo)
    }
    colnames(df)[1] = "hyperparameter"
    df[df=="NULL"] = NA
  }
  #print(df)
  algorithm = c(getLearnerShortName(lrn.par.set[[i]]$learner), rep("", nrow(df)-1))
  df = cbind(algorithm, df)
  index[i] = nrow(df)
  df_total = rbind(df_total, df)
}
df_total$trafo$mtry = c("$x \\cdot p$")
df_total$trafo$min.node.size = c("$n^x$")
df_total$hyperparameter = gsub("_", "\\_", df_total$hyperparameter, fixed = TRUE)

#df_total
  print(xtable(df_total, caption = "Hyperparameters of the algorithms. $p$ refers to the number of variables and $n$ to the
    number of observations. The used algorithms are  \\texttt{glmnet} \\citep{glmnet}, \\texttt{rpart} \\citep{rpart}, \\texttt{kknn} \\citep{kknn},  \\texttt{svm} \\citep{svm}, \\texttt{ranger} \\citep{ranger} and \\texttt{xgboost} \\citep{xgboost}.", digits = 0, label = "tab:parameter"),
    NA.string = "-",sanitize.text.function=function(x){gsub("_", "_",x, fixed = TRUE)},
    include.rownames=FALSE, hline.after = c(0, cumsum(index)))
@

These algorithms are run on a subset of the OpenML100 benchmark suite \citep{Bischl2017}, which consists of 100 classification datasets, carefully curated from the thousands of datasets available on OpenML \citep{OpenML2013}.
We only include datasets without missing data and with a binary outcome resulting in 38 datasets.
The datasets and their respective characteristics can be found in Table~\ref{tab:datasets}.

<<datasets, echo = FALSE, results='asis',  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
  library(OpenML)
  tasks = listOMLTasks(number.of.classes = 2L, number.of.missing.values = 0, tag = "study_14", estimation.procedure = "10-fold Crossvalidation", status = "active")
  tasks2 = listOMLTasks(number.of.classes = 2L, number.of.missing.values = 0, tag = "study_14", estimation.procedure = "10-fold Crossvalidation", status = "deactivated")
  tasks2 = tasks2[, -ncol(tasks2)]
  tasks = rbind(tasks, tasks2)
  tasks = tasks[, c("data.id", "task.id", "name", "number.of.instances", "number.of.features", "majority.class.size",
    "number.of.numeric.features", "number.of.symbolic.features")]
  #tasks$majority.class.size = tasks$majority.class.size / tasks$number.of.instances
  colnames(tasks) = c("Data_id", "Task_id", "Name", "n", "p", "majPerc", "numFeat", "catFeat")
  tasks$majPerc = tasks$majPerc/tasks$n

  load("./data/nr_results.RData")

  nr_results = nr_results[,which(colnames(nr_results) %in% sort(tasks$Data_id))]
  # delete these results from the database?
  # musk with easy perfect classification and two test datasets
  tasks = tasks[which(tasks$Data_id %in% colnames(nr_results)), ]
  # no results for some datasets
  tasks = tasks[order(tasks$Data_id),]
  #tasks$Data_id == colnames(nr_results)
  tasks = cbind(tasks, t(nr_results))
  colnames(tasks)[9:14] = c("glmnet", "rpart", "kknn", "svm", "ranger", "xgboost")
  tab = tasks[, c("Data_id", "Task_id", "Name", "n", "p", "majPerc", "numFeat", "catFeat")]

  print(xtable(tab, caption = "Included datasets and respective characteristics. \\textit{n} are the number of observations, \\textit{p} the number of features, \\textit{maj.class} the percentage of observations in the largest class, \\textit{numFeat} the number of numeric features and \\textit{catFeat} the number of categorical features.", digits = 2, label = "tab:datasets"), digits = 0, include.rownames=FALSE, size = "scriptsize")
@


\section{Random Experimentation Bot}

To conduct a large number of experiments a bot was implemented to automatically plan and execute runs, following the paradigm of random search.
The bot iteratively executes these steps: 

\newpage

\begin{enumerate}
\item Randomly sample a task $T$ (with an associated data set) from Table~\ref{tab:datasets}.
\item Randomly sample one ML algorithm $A$.
\item Randomly sample a hyperparameter setting $\theta$ of algorithm $A$, uniformly from the ranges specified in Table~\ref{tab:parameter},
then transform, if a transformation function is given.
\item Obtain task $T$ (and dataset) from OpenML and store it locally.
\item Evaluate algorithm $A$ with configuration $\theta$ on task $T$, with associated 10-fold cross-validation from OpenML.
\item Upload run results to OpenML, including hyperparameter configuration and time measurements.
\item OpenML now calculates various performance metrics for the uploaded cross-validated predictions.
\item The OpenML-ID of the bot (2702)  and the tag \texttt{mlrRandomBot} is used for identification.
\end{enumerate}

A clear advantage of random sampling is that all bot runs are completely independent of each other, making all experiments embarrassingly parallel.
Furthermore, more experiments can easily and conveniently added later on, without introducing any kind of bias into the sampling method.

The bot is developed open source in R and can be found on GitHub\footnote{\url{https://github.com/ja-thomas/OMLbots}}.
The bot is based on the R packages \texttt{mlr} \citep{Bischl2016} and \texttt{OpenML} \citep{Casalicchio2017} and written in modular form such that it can be extended with new sampling strategies for hyperparameters, algorithms and datasets in the future. Parallelization was performed with R package \texttt{batchtools} \citep{Lang2017}.

After more than $6$ million benchmark experiments the results of the bot are downloaded from OpenML.
For each of the algorithms $500000$ experiments are used to obtain the final dataset.
The experiments are chosen by the following procedure: For each algorithm, a threshold $B$ is set (see below) and, if the number of results for a dataset exceeds $B$, we draw randomly $B$ of the results obtained for this algorithm and this dataset. The threshold value $B$ is chosen for each algorithm separately to exactly obtain in total 500000 results for each algorithm.

For \texttt{kknn} we only execute 30 experiments per dataset because this number of experiments is high enough
to cover the hyperparameter space (that only consists of the parameter $k$ for $k \in \{1,...,30\}$) appropriately, resulting in 1140 experiments.
All in all this results in around 2.5 million experiments.

The distribution of the runs on the datasets and algorithms is displayed in Table~\ref{tab:datasets2}.

<<tbl_results, echo = FALSE, results='asis',  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
tab = tasks[, c("Data_id", "Task_id", "glmnet", "rpart", "kknn", "svm", "ranger", "xgboost")]
  tab = cbind(tab, rowSums(tab[,3:8]))
  tab = rbind(tab, colSums(tab))
  tab[nrow(tab),1] = "Total"
  colnames(tab)[ncol(tab)] = "Total"

  print(xtable(tab, caption = "Number of experiments for each combination of dataset and algorithm.", digits = 0, label = "tab:datasets2"), digits = 0, include.rownames=FALSE, size = "scriptsize", hline.after = c(-1, 0, nrow(tab)-1, nrow(tab)))
@

\section{Access to the results}
The results of the benchmark can be accessed in different ways:

\begin{itemize}
\item The easiest way to access them is to go to the figshare repository \citep{Kuehn2018} and to download the \texttt{.csv} files. For each algorithm there is a csv file that contains a row for each algorithm run with the columns \texttt{Data\_id}, the hyperparameter settings, the performance measures (auc, accuracy and brier score), the runtime, the scimark reference runtime and some characteristics of the dataset such as  the number of features or the number of observations.
\item Alternatively the code for the extraction of the data from the nightly database snapshot of OpenML can
be found here: \url{https://github.com/ja-thomas/OMLbots/blob/master/snapshot_database/database_extraction.R}. With this script all results that were created by the random bot (OpenML-ID 2702) are downloaded and the final dataset is created. (Warning: As the OpenML database is updated daily, changes can occur.)
\end{itemize}

\section{Discussion and potential usage of the results}

The presented data can be used to study the effect and influence of hyperparameter setting on performance in various ways.
Possible applications are:
\begin{itemize}
\item Obtaining defaults for ML algorithm that work well across many datasets \citep{Probst2018};
\item Measuring the importance of hyperparameters, to investigate which should be tuned \citep[see][]{Rijn2017, Probst2018};
\item Obtaining ranges or priors of tuning parameters to focus on important regions of the search space \citep[see][]{Rijn2017, Probst2018};
\item Meta-Learning;
\item Investigating, debugging and improving the robustness of algorithms.
\end{itemize}

Possible weaknesses of the approach, which we would like to address in the future, are:
\begin{itemize}
\item For each ML algorithm, a set of considered hyperparameters and their initial ranges has to be provided. It would be much more convenient if the bot could handle the set of all technical hyperparameters, with infinite ranges.
\item Smarter, sequential sampling might be required to scale to high-dimensional hyperparameter spaces.
But note that we not only care about optimal configurations but much rather would like to learn as much as possible about the considered parameter space, including areas of bad performance.
So simply switching to Bayesian optimization or related search techniques might not be appropriate.
\end{itemize}

%A potential weakness of this dataset is that the dimension of hyperparameter spaces for example for \texttt{xgboost} can be very high and the number of experiments is not sufficient to explore the space appropriately, especially in regions in which very high performance can be achieved.

\section*{References}

\bibliography{bot}

\end{document}
