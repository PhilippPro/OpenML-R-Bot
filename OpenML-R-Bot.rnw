

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm,amsmath,amssymb,amstext} 
\usepackage{booktabs}
\usepackage{array}
 
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\usepackage{hyperref}
\hypersetup{%
  colorlinks=true, 
  breaklinks=true, 
  urlcolor=blue, 
  linkcolor=blue, 
  citecolor=blue, 
}

\usepackage{nicefrac}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{setspace}


\begin{document}
\SweaveOpts{concordance=TRUE}

% !TeX root = RJwrapper.tex
% !TeX root = RJwrapper.tex
\title{Automatic Exploration of Machine Learning Experiments on OpenML}
\author{by Daniel KÃ¼hn*, Philipp Probst*, Janek Thomas and Bernd Bischl}

\maketitle

\section*{Abstract}
For any machine learning problem it is essential to have the right dataset. Meta-learning is a branch of machine learning, that focuses on hyperparameter optimization of machine learning algorithms. To date no large, open datset exists to support and benchmark meta-learning approaches. With this work we provide one such dataset for six different machine learning algorithms, which not only can be used for meta-learning but also for the better understanding of the explored hyperparameter space of these algorithms. 

\section{Introduction}

When applying machine learning on real world datasets, users have to choose from a large selection of different machine learning algorithms with many of these algorithms offering a set of hyperparameters, which can be specified by the user and can have a significant influence on the performance of the algorithm. Since there is no free lunch in algorithm selection and one can not expect one algorithm to outperform all the others \citep{Wolpert2001}, a crucial question practitioners have to face on a daily basis therefore is the selection of the "right" algorithm with the "right" hyperparameters for a given dataset. Several approaches like meta-learning or bayesian optimization exist for solving this issue. While showing promising results, bayesian optimization and other hyperparameter optimization techniques require a large overhead. Meta-learning has shown to decrease this overhead \citep{Feurer2015}, but no large and open source meta-learning datasets exist to date.
In this paper we describe how we created such a dataset by executing random machine learning experiments and storing the results on OpenML \citep{OpenML2013}, an open source database for machine learning problems. As large datasets like Imagenet \citep{ImageNet2009} have shown to improve the progress of machine learning, we hope to support the development of new meta-learning algorithms with this dataset. 

\section{Description of the bot}

We specified six machine learning algorithms with their specific hyperparameters.
We choosed the six supervised learning algorithms elastic net (\texttt{glmnet} package), 
decision tree (\texttt{rpart}), k-nearest neighbors (\texttt{kknn}), 
support vector machines (\texttt{svm}), random forest (\texttt{ranger}) 
and gradient boosting (\texttt{xgboost}). They represent algorithms that are used very 
often and cover a broad range of different types of algorithms. 
The algorithms with their specific hyperparameters and hyperparameter constraints can 
be found in table \ref{tab:parameter}. Some hyperparameters are transformed after they have been drawn, 
for example to get more results in regions where we expect more interesting results. 

<<parameter, echo = FALSE, results=tex,  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
library(xtable)
suppressWarnings(library(mlr))
load("./data/parameter_settings.RData")
df_total = data.frame()
index = numeric()
for (i in 1:6) {
  df = do.call(rbind, lrn.par.set[[i]]$param.set$pars)[,c("id", "type", "lower", "upper", "trafo")]
  if (is.null(nrow(df))) {
    df$trafo = NA 
    df = data.frame(df)
    colnames(df)[1] = "hyperparameter"
  } else {
    if (!all(data.frame(df)$trafo == "NULL")) {
      df = data.frame(df)
      df$trafo = lapply(df$trafo, function(x) if (!is.null(x)) paste0("$", deparse(x)[2], "$"))
      df$type[df$type == "numericvector"] = "numeric"
    } else { 
      df = data.frame(df)
      #df = subset(df, select = -trafo) 
    }
    colnames(df)[1] = "hyperparameter"
    df[df=="NULL"] = NA
  }
  #print(df)
  algorithm = c(getLearnerShortName(lrn.par.set[[i]]$learner), rep("", nrow(df)-1))
  df = cbind(algorithm, df)
  index[i] = nrow(df)
  df_total = rbind(df_total, df)
}
#df_total
  print(xtable(df_total, caption = "Hyperparameters of the algorithms", digits = 0, label = "tab:parameter"), 
    NA.string = "-",sanitize.text.function=function(x){gsub("_", "\\_",x, fixed = TRUE)}, 
    include.rownames=FALSE, hline.after = c(0, cumsum(index)))
@

Furthermore we choosed the classification tasks (datasets) from the OpenML100 
Benchmarksuite \citep{Bischl2017} as benchmark datasets, only including datasets without 
missing data and with binary outcome resulting in 76 datasets.
The datasets with their specific characteristics can be found in table \ref{tab:datasets}. 

<<datasets, echo = FALSE, results=tex,  fig.pos='htb!' , message = FALSE, fig.lp="fig:">>=
  library(OpenML)
  tasks = listOMLTasks(number.of.classes = 2L, number.of.missing.values = 0, 
    data.tag = "study_14", estimation.procedure = "10-fold Crossvalidation")
  tasks = tasks[, c("task.id", "name", "number.of.instances", "number.of.features")] 
  #tasks$majority.class.size = tasks$majority.class.size / tasks$number.of.instances
  colnames(tasks) = c("Task_id", "Name", "nObs", "nFeat")
  
  load("./data/nr_results.RData")
  
  nr_results = nr_results[,which(colnames(nr_results) %in% sort(tasks$Task_id))]
  # delete these results from the database?
  # musk with easy perfect classification and two test datasets
  tasks = tasks[which(sort(tasks$Task_id) %in% colnames(nr_results)), ]
  # no results for some datasets
  tasks = tasks[order(tasks$Task_id),]
  #tasks$Task_id == colnames(nr_results)
  tasks = cbind(tasks, t(nr_results))
  colnames(tasks)[5:10] = c("glmnet", "rpart", "kknn", "svm", "ranger", "xgboost")
  
  print(xtable(tasks, caption = "Included datasets, number of observations (nObs), 
    number of features (nFeat) and number of runs of each algorithm", digits = 0, label = "tab:data"), digits = 0, include.rownames=FALSE, size = "scriptsize")
@

Following the search paradigm of random search the bot iteratively executes several steps:
\begin{enumerate}
\item Randomly draw one of the six algorithms
\item Randomly draw a hyperparameter setting of the chosen algorithm
\item Randomly draw one of the benchmark datasets
\item Download the dataset from OpenML
\item Benchmark the specified algorithm on the specified dataset with 10-fold cross-validation
\item Upload the benchmark results with time measurements to OpenML with the 
identification tag \texttt{mlrRandomBot}
\end{enumerate}

The code for the bot can be found on github (\url{https://github.com/ja-thomas/OMLbots}), the R packages \texttt{mlr} \citep{Bischl2016} 
and \texttt{OpenML} \citep{Casalicchio2017} were used for the whole process. 

In total more than 6 millions (XXX) runs of the random bot on different 
server platforms (OH NO!) like Azure were executed.

\section{Access to the benchmark results}

The results of the benchmarks can be accessed in different ways:

\begin{itemize}
\item The easiest way to access them is to go to the figshare repository \citep{Probst2017} and
download the .csv files. 
\end{itemize}

\section{Validity of the results}

DISTRIBUTION OF HYPERPARAMETERS AND DATASETS

\section{Potential usage of the results}

The results can be used to discover effects of the hyperparameters on performances 
of the different algorithms on different datasets. 

This can be used to:
\begin{itemize}
\item Find good defaults for the algorithms that work well on many datasets
\item Measure differences between the algorithms
\item Optimize tuning algorithms:
\begin{itemize}
\item Measure the tunability of algorithms and find out which parameters should be tuned
\item Use the results to get priors for tuning algorithms - in which regions of the 
hyperparameter space should be searched with higher probability?
\end{itemize}
\item Meta-Learning: Train models that based on dataset characteristics and 
possibly time limitations propose hyperparameter settings that perform good 
on a specific dataset
\end{itemize}

\section{Literature}
Other related projects and papers (e.g. AutoWeka)

\section{to-Do}
\begin{itemize}
\item Think about the title
\item Weakness: Dimension is too high, e.g. for xgboost. The best regions are not explored enough. 
Maybe add later. 
\end{itemize}

\bibliography{bot}
 
\end{document}