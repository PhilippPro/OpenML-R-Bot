

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm,amsmath,amssymb,amstext} 
\usepackage{booktabs}
\usepackage{array}
 
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\usepackage{hyperref}
\hypersetup{%
  colorlinks=true, 
  breaklinks=true, 
  urlcolor=blue, 
  linkcolor=blue, 
  citecolor=blue, 
}

\usepackage{etoolbox}
\appto\UrlBreaks{\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j
\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w
\do\x\do\y\do\z}


\usepackage{nicefrac}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{setspace}


\usepackage{Sweave}
\begin{document}
\input{OpenML-R-Bot-concordance}


\title{Automatic Exploration of Machine Learning Experiments on OpenML}
\author{by Daniel KÃ¼hn*, Philipp Probst*, Janek Thomas and Bernd Bischl}

\maketitle

\section*{Abstract}
Understanding the influence of hyperparameters on the performance of a machine learning algorithm is an important part of finding a well performing and adequately tuned algorithm for a given dataset. As to date no dataset exists to support this required understanding for state of the art algorithms like gradient boosting or random forest, this paper presents a large and open dataset for this problem (see \citep{Probst2018} to access the dataset). The dataset contains the performance results  (AUC, accuracy and Brier score) of six different machine learning algorithms, the used hyperparameters that were set by a random search, runtimes and meta-data for each dataset. It can be used for meta-learning, finding good defaults, measuring the tunability of algorithms and hyperparameters, benchmarking of algorithms and other tasks mainly related to hyperparameters. %The contained algorithms are elastic net (glmnet R-package), decision tree (rpart), k-nearest neighbors (kknn), support vector machine (e1071), random forest (ranger) and extreme gradient boosting (xgboost).


\section{Introduction}

When applying machine learning on real world datasets, users have to choose from a large selection of different machine learning algorithms with many of these algorithms offering a set of hyperparameters, which can be specified by the user and can have a significant influence on the performance of the algorithm. Since there is no free lunch in algorithm selection and one can not expect one algorithm to outperform all the others \citep{Wolpert2001}, a crucial question practitioners have to face on a daily basis therefore is the selection of the "right" algorithm with the "right" hyperparameters for a given dataset. This problem is not easy to solve, because choices are many, the evaluation of a single machine learning run usally is computationally expensive and the hyperparameter-space is complex \citep{Claesen2015}. A usual approach is to run a tuning algorithm like bayesian optimization to find the best hyperparameter setting. While showing promising results, bayesian optimization and other hyperparameter optimization techniques require a large overhead. 

Meta-learning tries to decrease this overhead \citep{Feurer2015}, by using information of previous algorithm runs on other datasets. A requirement for this, is to have a meta-learning dataset, that contains the information of previous runs. With this paper we provide a open accesible dataset that contains information of previous runs of six different machine learning algorithms on 38 classification datasets. As large datasets like Imagenet \citep{ImageNet2009} have shown to improve the progress of machine learning, we hope to support the development of new meta-learning algorithms with this dataset. While similar meta-datasets were created in the past, we were not able to access them by the links provided in their respective papers: \citet{2014arXiv1405.7292S} provide a repository of with weka-based machine learning experiments on 72 data sets, 9 machine learning algorithms, 10 hyperparameter settings for each algorithm, and several meta-features of each data set. \citet{Reif2012ACD} also created a meta-dataset based on machine learning experiments on 83 datasets, 6 classification algorithms, and 49 meta-features. 

We first describe how we created such a meta-dataset by executing random machine learning experiments and storing the results on OpenML \citep{OpenML2013}, an open source database for machine learning problems. Then the possibilities of accessing this dataset are shortly presented. Finally we briefly discuss potential usage of the dataset. 


\section{Creating the dataset}

\subsection{Algorithms}

To create the dataset the following six supervised machine learning algorithms were run with predefined ranges for their relevant hyperparameters on 38 classification tasks: elastic net (\texttt{glmnet}), decision tree (\texttt{rpart}), k-nearest neighbors (\texttt{kknn}), support vector machines (\texttt{svm}), random forest (\texttt{ranger}) and gradient boosting (\texttt{xgboost}). These algorithms cover a broad range of approaches to machine learning. For each algorithm the available hyperparameters were explored in a predefined range (see table~\ref{tab:parameter}). Values of some of these ranges were transformed by the function found in column \textit{trafo} to explore the range in a non-uniform manner. This is useful, if e.g. minor changes in a hyperparameter are not expected to have a significant impact on the performance of an algorithm.

% latex table generated in R 3.4.3 by xtable 1.8-2 package
% Tue May 08 23:48:31 2018
\begin{table}[ht]
\centering
\begin{tabular}{llrrrr}
 algorithm & hyperparameter & type & lower & upper & trafo \\ 
  \hline
glmnet & alpha & numeric & 0 & 1 & - \\ 
   & lambda & numeric & -10 & 10 & $2^x$ \\ 
   \hline
rpart & cp & numeric & 0 & 1 & - \\ 
   & maxdepth & integer & 1 & 30 & - \\ 
   & minbucket & integer & 1 & 60 & - \\ 
   & minsplit & integer & 1 & 60 & - \\ 
   \hline
kknn & k & integer & 1 & 30 & - \\ 
   \hline
svm & kernel & discrete & - & - & - \\ 
   & cost & numeric & -10 & 10 & $2^x$ \\ 
   & gamma & numeric & -10 & 10 & $2^x$ \\ 
   & degree & integer & 2 & 5 & - \\ 
   \hline
ranger & num.trees & integer & 1 & 2000 & - \\ 
   & replace & logical & - & - & - \\ 
   & sample.fraction & numeric & 0 & 1 & - \\ 
   & mtry & numeric & 0 & 1 & $x \cdot p$ \\ 
   & respect.unordered.factors & logical & - & - & - \\ 
   & min.node.size & numeric & 0 & 1 & $n^x$ \\ 
   \hline
xgboost & nrounds & integer & 1 & 5000 & - \\ 
   & eta & numeric & -10 & 0 & $2^x$ \\ 
   & subsample & numeric & 0 & 1 & - \\ 
   & booster & discrete & - & - & - \\ 
   & max\_depth & integer & 1 & 15 & - \\ 
   & min\_child\_weight & numeric & 0 & 7 & $2^x$ \\ 
   & colsample\_bytree & numeric & 0 & 1 & - \\ 
   & colsample\_bylevel & numeric & 0 & 1 & - \\ 
   & lambda & numeric & -10 & 10 & $2^x$ \\ 
   & alpha & numeric & -10 & 10 & $2^x$ \\ 
   \hline
\end{tabular}
\caption{Hyperparameters of the algorithms. $p$ refers to the number of variables and $n$ to the 
    number of observations.} 
\label{tab:parameter}
\end{table}
\subsection{Datasets}

These algorithms were run on a subset of the OpenML100 Benchmark suite \citep{Bischl2017}, which consists of 100 classification datasets
carefully curated from the thousands of datasets available on OpenML. We only included datasets without 
missing data and with a binary outcome resulting in 38 datasets.
The datasets with their specific characteristics can be found in table~\ref{tab:datasets}.  

% latex table generated in R 3.4.3 by xtable 1.8-2 package
% Tue May 08 23:48:35 2018
\begin{table}[ht]
\centering
\begingroup\scriptsize
\begin{tabular}{rlrrrrr}
  \hline
Data\_id & Name & nObs & nFeat & majPerc & numFeat & catFeat \\ 
  \hline
  3 & kr-vs-kp & 3196 &  37 & 0.52 &   0 &  37 \\ 
   31 & credit-g & 1000 &  21 & 0.70 &   7 &  14 \\ 
   37 & diabetes & 768 &   9 & 0.65 &   8 &   1 \\ 
   44 & spambase & 4601 &  58 & 0.61 &  57 &   1 \\ 
   50 & tic-tac-toe & 958 &  10 & 0.65 &   0 &  10 \\ 
  151 & electricity & 45312 &   9 & 0.58 &   7 &   2 \\ 
  312 & scene & 2407 & 300 & 0.82 & 294 &   6 \\ 
  333 & monks-problems-1 & 556 &   7 & 0.50 &   0 &   7 \\ 
  334 & monks-problems-2 & 601 &   7 & 0.66 &   0 &   7 \\ 
  335 & monks-problems-3 & 554 &   7 & 0.52 &   0 &   7 \\ 
  1036 & sylva\_agnostic & 14395 & 217 & 0.94 & 216 &   1 \\ 
  1038 & gina\_agnostic & 3468 & 971 & 0.51 & 970 &   1 \\ 
  1046 & mozilla4 & 15545 &   6 & 0.67 &   5 &   1 \\ 
  1049 & pc4 & 1458 &  38 & 0.88 &  37 &   1 \\ 
  1050 & pc3 & 1563 &  38 & 0.90 &  37 &   1 \\ 
  1063 & kc2 & 522 &  22 & 0.80 &  21 &   1 \\ 
  1067 & kc1 & 2109 &  22 & 0.85 &  21 &   1 \\ 
  1068 & pc1 & 1109 &  22 & 0.93 &  21 &   1 \\ 
  1120 & MagicTelescope & 19020 &  12 & 0.65 &  11 &   1 \\ 
  1176 & Internet-Advertisements & 3279 & 1559 & 0.86 & 1558 &   1 \\ 
  1220 & Click\_prediction\_small & 39948 &  12 & 0.83 &  11 &   1 \\ 
  1461 & bank-marketing & 45211 &  17 & 0.88 &   7 &  10 \\ 
  1462 & banknote-authentication & 1372 &   5 & 0.56 &   4 &   1 \\ 
  1464 & blood-transfusion-service-center & 748 &   5 & 0.76 &   4 &   1 \\ 
  1467 & climate-model-simulation-crashes & 540 &  21 & 0.91 &  20 &   1 \\ 
  1471 & eeg-eye-state & 14980 &  15 & 0.55 &  14 &   1 \\ 
  1479 & hill-valley & 1212 & 101 & 0.50 & 100 &   1 \\ 
  1480 & ilpd & 583 &  11 & 0.71 &   9 &   2 \\ 
  1485 & madelon & 2600 & 501 & 0.50 & 500 &   1 \\ 
  1486 & nomao & 34465 & 119 & 0.71 &  89 &  30 \\ 
  1487 & ozone-level-8hr & 2534 &  73 & 0.94 &  72 &   1 \\ 
  1489 & phoneme & 5404 &   6 & 0.71 &   5 &   1 \\ 
  1504 & steel-plates-fault & 1941 &  34 & 0.65 &  33 &   1 \\ 
  1510 & wdbc & 569 &  31 & 0.63 &  30 &   1 \\ 
  1570 & wilt & 4839 &   6 & 0.95 &   5 &   1 \\ 
  4134 & Bioresponse & 3751 & 1777 & 0.54 & 1776 &   1 \\ 
  4135 & Amazon\_employee\_access & 32769 &  10 & 0.94 &   0 &  10 \\ 
  4534 & PhishingWebsites & 11055 &  31 & 0.56 &   0 &  31 \\ 
   \hline
\end{tabular}
\endgroup
\caption{Included datasets with meta-data. \textit{nObs} are the number of observations, \textit{nFeat} the number of Features, \textit{majPerc} the percentage of observations with the most common class, \textit{numFeat} the number of numeric features and \textit{catFeat} the number of categorical features.} 
\label{tab:datasets}
\end{table}
\subsection{Execution of the bot}

Following the search paradigm of random search the bot iteratively executes several steps:
\begin{enumerate}
\item Randomly draw one of the six algorithms
\item Randomly draw a hyperparameter setting of the chosen algorithm
\item Randomly draw one of the 38 binary classification benchmark datasets
\item Download the dataset from OpenML (or from cache)
\item Benchmark the specified algorithm on the specified dataset with 10-fold cross-validation (the standardized splits for the cross-validation are specified by OpenML)
\item Upload the benchmark results with hyperparameters, performance measures and time measurements to OpenML with the tag \texttt{mlrRandomBot} used for identification
\end{enumerate}


An advantage of using random search instead of other methods like grid search is that further experiments can be easily added to the existing ones. One could for example easily increase the hyperparameter space for a specific algorithm or add a complete new algorithm by just adding new experiments to the existing ones. 

The code for the bot can be found on GitHub (\url{https://GitHub.com/ja-thomas/OMLbots}). Further algorithms can be easily added by adding an algorithm with specific hyperparameters in the \texttt{R/botSetLearnerParamPairs.R} file of the GitHub repository. The main function \texttt{runBot} executes the bot with specific settings (number of experiments, temporary file, sample configurations, etc.).

The R packages \texttt{mlr} \citep{Bischl2016} 
and \texttt{OpenML} \citep{Casalicchio2017} were used for the whole process. 


\subsection{Extraction of results}

After having run more than 6 million benchmark experiments the results of the bot are downloaded from OpenML. 
Because of technical reasons on one dataset (data.id = 4135) all algorithms except of \texttt{rpart}
and \texttt{ranger} provided errors, so we exclude it and 38 datasets are left.

For each of the algorithms we only take 500000 experiments to obtain a dataset which is balanced regarding the experiment runs.  
They are chosen by the following procedure: for each algorithm, a threshold $B$ is set (see below) and, if the number of results for a dataset exceeds $B$, we draw randomly $B$ of the results obtained for this algorithm and this dataset. For each algorithm, the threshold value $B$ is chosen for each algorithm separately to exactly obtain 500000 results for each algorithm. 

For \texttt{kknn} we only executed 30 experiments per dataset because this number of experiments is high enough
to cover the hyperparameter space (that only consists of the parameter $k$ for $k \in \{1,...,30\}$) appropriately, resulting in 1140 experiments.
In total this results in around 2.5 million experiments.

The distribution of the runs on the datasets and algorithms can be seen in table~\ref{tab:datasets2}.

% latex table generated in R 3.4.3 by xtable 1.8-2 package
% Tue May 08 23:48:35 2018
\begin{table}[ht]
\centering
\begingroup\scriptsize
\begin{tabular}{lrrrrrrr}
  \hline
Data\_id & glmnet & rpart & kknn & svm & ranger & xgboost & Total \\ 
  \hline
3 & 15547 & 14633 & 30 & 19644 & 15139 & 16867 & 81860 \\ 
  31 & 15547 & 14633 & 30 & 19644 & 15139 & 16867 & 81860 \\ 
  37 & 15546 & 14633 & 30 & 15985 & 15139 & 16866 & 78199 \\ 
  44 & 15547 & 14633 & 30 & 19644 & 15139 & 16867 & 81860 \\ 
  50 & 15547 & 14633 & 30 & 19644 & 15139 & 16866 & 81859 \\ 
  151 & 15547 & 14632 & 30 & 2384 & 12517 & 16866 & 61976 \\ 
  312 & 6613 & 13455 & 30 & 18740 & 12985 & 15886 & 67709 \\ 
  333 & 15546 & 14632 & 30 & 19644 & 15139 & 16867 & 81858 \\ 
  334 & 15547 & 14633 & 30 & 19644 & 14492 & 16867 & 81213 \\ 
  335 & 15547 & 14633 & 30 & 15123 & 15139 & 10002 & 70474 \\ 
  1036 & 14937 & 14633 & 30 & 2338 & 7397 & 2581 & 41916 \\ 
  1038 & 15547 & 5151 & 30 & 5716 & 4827 & 1370 & 32641 \\ 
  1046 & 6466 & 14633 & 30 & 10121 & 3788 & 16867 & 51905 \\ 
  1049 & 15547 & 14633 & 30 & 5422 & 8842 & 11812 & 56286 \\ 
  1050 & 7423 & 14632 & 30 & 12064 & 15139 & 4453 & 53741 \\ 
  1063 & 15547 & 14633 & 30 & 19644 & 11357 & 13758 & 74969 \\ 
  1067 & 15547 & 14633 & 30 & 19644 & 7914 & 16866 & 74634 \\ 
  1068 & 15546 & 14632 & 30 & 10229 & 7386 & 16866 & 64689 \\ 
  1120 & 15546 & 14633 & 30 & 13893 & 8173 & 16866 & 69141 \\ 
  1176 & 15531 & 7477 & 30 & 3908 & 9760 & 8143 & 44849 \\ 
  1220 & 13005 & 14632 & 30 & 14451 & 15140 & 13047 & 70305 \\ 
  1461 & 6970 & 14073 & 30 & 2678 & 14323 & 2215 & 40289 \\ 
  1462 & 8955 & 14633 & 30 & 6320 & 15139 & 16867 & 61944 \\ 
  1464 & 15547 & 14632 & 30 & 19644 & 15139 & 16867 & 81859 \\ 
  1467 & 15547 & 14633 & 30 & 4441 & 15139 & 16866 & 66656 \\ 
  1471 & 15547 & 14633 & 30 & 9725 & 13523 & 16866 & 70324 \\ 
  1479 & 15546 & 14633 & 30 & 19644 & 15140 & 16867 & 81860 \\ 
  1480 & 15024 & 14633 & 30 & 19644 & 15139 & 16254 & 80724 \\ 
  1485 & 8247 & 10923 & 30 & 10334 & 15139 & 9237 & 53910 \\ 
  1486 & 3866 & 11389 & 30 & 1490 & 15139 & 5813 & 37727 \\ 
  1487 & 15547 & 6005 & 30 & 19644 & 15139 & 11194 & 67559 \\ 
  1489 & 15547 & 14633 & 30 & 17298 & 15139 & 16867 & 79514 \\ 
  1504 & 15547 & 14632 & 30 & 19644 & 15139 & 16867 & 81859 \\ 
  1510 & 15547 & 14633 & 30 & 19644 & 15140 & 16867 & 81861 \\ 
  1570 & 15547 & 14633 & 30 & 19644 & 15139 & 16867 & 81860 \\ 
  4134 & 15546 & 14632 & 30 & 19644 & 15139 & 16867 & 81858 \\ 
  4135 & 1493 & 3947 & 30 & 560 & 14516 & 2222 & 22768 \\ 
  4534 & 2801 & 3231 & 30 & 2476 & 15139 & 947 & 24624 \\ 
   \hline
Total & 500000 & 500000 & 1140 & 500000 & 500000 & 500000 & 2501140 \\ 
   \hline
\end{tabular}
\endgroup
\caption{Number of results by dataset and algorithm} 
\label{tab:datasets2}
\end{table}
\section{Access to the benchmark results}

The results of the benchmarks can be accessed in different ways:

\begin{itemize}
\item The easiest way to access them is to go to the figshare repository \citep{Probst2018} and
download the \texttt{.csv} files or the \texttt{.RData} file. 
\item Alternatively the code for the extraction of the data from the nightly database snapshot of OpenML can 
be found here: \url{https://github.com/ja-thomas/OMLbots/blob/master/snapshot_database/database_extraction.R}
\end{itemize}

\section{Potential usage of the results and discussion}

The results can be used to discover effects of the hyperparameters on performances 
of the different algorithms on different datasets. 

This can be used to:
\begin{itemize}
\item Find good defaults for the algorithms that work well on many datasets
\item Measure differences between the algorithms
\item Optimize tuning algorithms:
\begin{itemize}
\item Measure the tunability of algorithms and find out which parameters should be tuned \citep[see][]{Probst20182}
\item Use the results to get priors for tuning algorithms - in which regions of the 
hyperparameter space should be searched with higher probability?
\end{itemize}
\item Meta-Learning: Train models that based on dataset characteristics and 
possibly time limitations propose hyperparameter settings that perform good 
on a specific dataset
\end{itemize}

A potential weakness of this dataset is that the dimension of the hyperparameters for e.g. \texttt{xgboost} is very high and the number of experiments is not high enough to explore the regions appropriately, especially regions where very high performance can be achieved. This could potentially be improved by using a tuning algorithm and by adding the results of the tuning steps to the existing database. 


\bibliography{bot}
 
\end{document}
