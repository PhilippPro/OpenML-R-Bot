

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsthm,amsmath,amssymb,amstext} 

\usepackage{array}
 

\bibliographystyle{abbrvnat}
\usepackage{hyperref}
\hypersetup{%
  colorlinks=true, 
  breaklinks=true, 
  urlcolor=blue, 
  linkcolor=blue, 
  citecolor=blue, 
}

\usepackage{nicefrac}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{colortbl}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{setspace}


\usepackage{Sweave}
\begin{document}
\input{OpenML-R-Bot-concordance}

% !TeX root = RJwrapper.tex
% !TeX root = RJwrapper.tex
\title{Automatic Exploration of Machine Learning Experiments on OpenML}
\author{by Daniel KÃ¼hn*, Philipp Probst*, Janek Thomas and Bernd Bischl}

\maketitle

\section{
Abstract
}

\section{Introduction}

When applying machine learning on real world datasets, users have to choose from a large selection of different machine learning algorithms with many of these algorithms offering a set of hyperparameters, which can be specified by the user and can have a significant influence on the performance of the algorithm. Since there is no free lunch in algorithm selection and one can not expect one algorithm to outperform all the others \citep{Wolpert2001}, a crucial question practitioners have to face on a daily basis therefore is the selection of the "right" algorithm with the "right" hyperparameters for a given dataset. 


In this paper we describe how experiments with the random bot were executed on OpenML \citep{OpenML2013}. 

\section{Description of the bot}

We specified six machine learning algorithms with their specific hyperparameters.
We choosed the six supervised learning algorithms elastic net (\texttt{glmnet} package), 
decision tree (\texttt{rpart}), k-nearest neighbors (\texttt{kknn}), 
support vector machines (\texttt{svm}), random forest (\texttt{ranger}) 
and gradient boosting (\texttt{xgboost}). They represent algorithms that are used very 
often and cover a broad range of different types of algorithms. 
The algorithms with their specific hyperparameters and hyperparameter constraints can 
be found in table \ref{tab:parameter}. Some hyperparameters are transformed after they have been drawn, 
for example to get more results in regions where we expect more interesting results. 



Furthermore we choosed the classification tasks (datasets) from the OpenML100 
Benchmarksuite \citep{Bischl2017} as benchmark datasets, only including datasets without 
missing data and with binary outcome resulting in 76 datasets.
The datasets with their specific characteristics can be found in table XXX. 

Following the search paradigm of random search the bot iterativel executes several steps:
\begin{enumerate}
\item Randomly draw one of the six algorithms
\item Randomly draw a hyperparameter setting of the chosen algorithms
\item Randomly draw one of the benchmark datasets
\item Download the dataset from OpenML
\item Benchmark the specified algorithm on the specified dataset with 10-fold cross-validation
\item Upload the benchmark results with time measurements to OpenML with the 
identification tag \texttt{mlrRandomBot}
\end{enumerate}

The code for the bot can be found on github (\url{https://github.com/ja-thomas/OMLbots}), the R packages \texttt{mlr} \citep{Bischl2016} 
and \texttt{OpenML} \citep{Casalicchio2017} were used for the whole process. 

In total more than 6 millions (XXX) runs of the random bot on different 
server platforms (OH NO!) like Azure were executed.

\section{Access to the benchmark results}

The results of the benchmarks can be accessed in different ways:

\begin{itemize}
\item The easiest way to access them is to go to the figshare repository \citep{Probst2017} and
download the .csv files. 
\end{itemize}

\section{Validity of the results}

DISTRIBUTION OF HYPERPARAMETERS AND DATASETS

\section{Potential usage of the results}

The results can be used to discover effects of the hyperparameters on performances 
of the different algorithms on different datasets. 

This can be used to:
\begin{itemize}
\item Find good defaults for the algorithms that work well on many datasets
\item Measure differences between the algorithms
\item Optimize tuning algorithms:
\begin{itemize}
\item Measure the tunability of algorithms and find out which parameters should be tuned
\item Use the results to get priors for tuning algorithms - in which regions of the 
hyperparameter space should be searched with higher probability?
\end{itemize}
\item Meta-Learning: Train models that based on dataset characteristics and 
possibly time limitations propose hyperparameter settings that perform good 
on a specific dataset
\end{itemize}


\section{Literature}
Other related projects and papers (e.g. AutoWeka)

\section{to-Do}
\begin{itemize}
\item Think about the title
\item Put the package on CRAN
\item Put the database on e.g. figshare
\end{itemize}

\bibliography{bot}
 
\end{document}
